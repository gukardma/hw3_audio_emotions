{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Emotion Detection\n",
    "\n",
    "In this project, you will adapt the original Cat vs. Dog project to work with the **RAVDESS_Emotional_speech_audio** dataset for emotion detection. Below is a summary of the key modifications you will need to make, along with instructions on what parts you need to complete:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Dataset and File Structure\n",
    "\n",
    "- **Original Project:**  \n",
    "  - Used a dataset with files labeled as either \"cat\" or \"dog\".\n",
    "\n",
    "- **New Emotion Dataset (RAVDESS):**  \n",
    "  - The dataset is organized into 24 subdirectories (e.g., `Actor_01`, `Actor_02`, …, `Actor_24`), each containing 60 `.wav` files.\n",
    "  - **Filename Format:**  \n",
    "    The file names follow this format:  \n",
    "    ```\n",
    "    Modality-VocalChannel-Emotion-EmotionalIntensity-Statement-Repetition-Actor.wav\n",
    "    ```\n",
    "    For example, `03-01-06-01-02-01-12.wav` means:  \n",
    "    - **Emotion Code:** The 3rd part (`06`) indicates the emotion (in this case, \"Fearful\").\n",
    "\n",
    "- **Your Task:**  \n",
    "  - Adapt the code to recursively load `.wav` files from the RAVDESS dataset.\n",
    "  - Parse the filename (split by `'-'`) to extract the emotion code.\n",
    "  \n",
    "---\n",
    "\n",
    "#### 2. Label Mapping\n",
    "\n",
    "- **Original Project:**  \n",
    "  - Labels were assigned based on the filename prefix (\"cat\" or \"dog\").\n",
    "\n",
    "- **New Emotion Labels:**  \n",
    "  - Create a mapping from the emotion code (from the filename) to a numerical label. For example:\n",
    "    - `\"01\"` → Neutral  \n",
    "    - `\"02\"` → Calm  \n",
    "    - `\"03\"` → Happy  \n",
    "    - `\"04\"` → Sad  \n",
    "    - `\"05\"` → Angry  \n",
    "    - `\"06\"` → Fearful  \n",
    "    - `\"07\"` → Disgust  \n",
    "    - `\"08\"` → Surprised\n",
    "\n",
    "- **Your Task:**  \n",
    "  - Use a dictionary (e.g., `emotion_map`) to map these codes to labels.\n",
    "  - Convert the labels to integers (e.g., 0 to 7).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Multi-class Classification\n",
    "\n",
    "- **Original Project:**  \n",
    "  - Solved a binary classification problem (cat vs. dog).\n",
    "\n",
    "- **New Emotion Detection:**  \n",
    "  - You now have 8 emotion classes.\n",
    "  - **Random Forest Classifier:**  \n",
    "    - Continue using statistical summaries of MFCC features, but ensure that the classifier is trained with multi-class labels.\n",
    "  - **Convolutional Neural Network (CNN):**  \n",
    "    - Change the output layer to have **8 neurons** with a **softmax activation** function.\n",
    "    - Update the loss function to `sparse_categorical_crossentropy` for multi-class classification.\n",
    "\n",
    "- **Your Task:**  \n",
    "  - Modify the CNN architecture accordingly.\n",
    "  - Verify that your evaluation metrics and confusion matrix display the 8 classes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Feature Extraction and Visualization\n",
    "\n",
    "- **Similarities:**  \n",
    "  - The process for extracting MFCC features (including deltas) remains largely the same.\n",
    "  - Visualization of the audio waveform and MFCC heatmap (using `imshow` or similar) is still applicable.\n",
    "\n",
    "- **Your Task:**  \n",
    "  - Adapt the feature extraction code to work with the RAVDESS dataset.\n",
    "  - Ensure that the visualization parts still help you verify the quality of the extracted features.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Additional Notebook for Inference on New Audio\n",
    "\n",
    "- **New Requirement:**  \n",
    "  - Create a **separate notebook** where you:\n",
    "    - Record your own voice using the microphone.\n",
    "    - Apply the trained model (which you saved from this project) to predict the emotion in your recording.\n",
    "  \n",
    "- **Your Task:**  \n",
    "  - Build a new notebook that includes:\n",
    "    - Code to record audio from the microphone.\n",
    "    - Feature extraction code (to compute MFCCs from your recording).\n",
    "    - Code to load the pre-trained model (Random Forest and/or CNN) and output the predicted emotion.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import sounddevice as sd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion mapping: convert emotion code (string) to integer label (0-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    \"01\": 0,  # Neutral\n",
    "    \"02\": 1,  # Calm\n",
    "    \"03\": 2,  # Happy\n",
    "    \"04\": 3,  # Sad\n",
    "    \"05\": 4,  # Angry\n",
    "    \"06\": 5,  # Fearful\n",
    "    \"07\": 6,  # Disgust\n",
    "    \"08\": 7   # Surprised\n",
    "}\n",
    "\n",
    "emotion_labels = [\"Neutral\", \"Calm\", \"Happy\", \"Sad\", \"Angry\", \"Fearful\", \"Disgust\", \"Surprised\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract statistical MFCC features (for Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_features(file_path, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        mfcc_std = np.std(mfcc, axis=1)\n",
    "        delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "        delta_std = np.std(mfcc_delta, axis=1)\n",
    "        delta2_mean = np.mean(mfcc_delta2, axis=1)\n",
    "        delta2_std = np.std(mfcc_delta2, axis=1)\n",
    "        features = np.concatenate([mfcc_mean, mfcc_std, delta_mean, delta_std, delta2_mean, delta2_std])\n",
    "        return features, y, sr, mfcc\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to extract fixed-size MFCC image for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_image(file_path, n_mfcc=13, max_len=216):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        # Pad or truncate the MFCC to have a fixed number of frames (max_len)\n",
    "        mfcc_fixed = librosa.util.fix_length(mfcc, size=max_len, axis=1)\n",
    "        return mfcc_fixed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset path and recursively get all .wav files from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../Codes/datasets/RAVDESS_Emotional_speech_audio'\n",
    "\n",
    "audio_files = glob.glob(os.path.join(dataset_path, '**', '*.wav'), recursive=True)\n",
    "print(\"Number of audio files found:\", len(audio_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize lists for features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "labels_list = []\n",
    "waveforms = []   # For visualization\n",
    "sample_rates = []  # For visualization\n",
    "mfccs = []       # For visualization\n",
    "file_names = []  # To store file paths\n",
    "\n",
    "# For CNN images\n",
    "cnn_images = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process each audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in audio_files:\n",
    "    # Extract statistical features for Random Forest\n",
    "    features, y, sr, mfcc = extract_statistical_features(file_path, n_mfcc=13)\n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "        waveforms.append(y)\n",
    "        sample_rates.append(sr)\n",
    "        mfccs.append(mfcc)\n",
    "        file_names.append(file_path)\n",
    "        \n",
    "        # Parse the filename to extract the emotion code (3rd part of the filename)\n",
    "        base_name = os.path.basename(file_path)\n",
    "        parts = base_name.split('-')\n",
    "        if len(parts) >= 3:\n",
    "            emotion_code = parts[2]\n",
    "            label = emotion_map.get(emotion_code, -1)  # default to -1 if not found\n",
    "            labels_list.append(label)\n",
    "        else:\n",
    "            labels_list.append(-1)\n",
    "        \n",
    "        # Extract MFCC image for CNN classifier\n",
    "        mfcc_img = get_mfcc_image(file_path, n_mfcc=13, max_len=216)\n",
    "        if mfcc_img is not None:\n",
    "            cnn_images.append(mfcc_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert lists to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = np.array(features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "cnn_images = np.array(cnn_images)\n",
    "\n",
    "print(\"Features array shape (Random Forest):\", features_array.shape)\n",
    "print(\"Labels array shape:\", labels_array.shape)\n",
    "print(\"CNN images shape:\", cnn_images.shape)  # Expected shape: (num_samples, 13, 216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize an Example Audio File and its MFCC Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the audio waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the MFCC heatmap using imshow (as an alternative to specshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data and Train the CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand dimensions of CNN images to add a channel dimension (required by CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "test = [[28,28],[28,28]]\n",
    "test_tensor = np.array(test)\n",
    "\n",
    "test_tensor = tf.expand_dims(test_tensor, axis=1)\n",
    "\n",
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_images_exp = cnn_images[..., np.newaxis]  # New shape: (num_samples, 13, 216, 1)\n",
    "cnn_images_exp = tf.expand_dims(cnn_images, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data for the CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the CNN model for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the CNN model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on a New Audio File or via Microphone Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set source = 'file' to use an external audio file (e.g., 'new_audio.wav')\n",
    "# Set source = 'mic' to record audio from the microphone\n",
    "\n",
    "source = 'mic'  # Change to 'file' if you want to use an external file\n",
    "\n",
    "if source == 'file':\n",
    "    # Inference using an external audio file\n",
    "    new_audio_path = 'new_audio.wav'  # Provide the path to your audio file\n",
    "    \n",
    "    # Extract features for the Random Forest model\n",
    "    features_new, y_new, sr_new, mfcc_new = extract_statistical_features(new_audio_path, n_mfcc=13)\n",
    "    if features_new is not None:\n",
    "        features_new = features_new.reshape(1, -1)\n",
    "    \n",
    "    # Extract MFCC image for the CNN model\n",
    "    mfcc_img_new = get_mfcc_image(new_audio_path, n_mfcc=13, max_len=216)\n",
    "    if mfcc_img_new is not None:\n",
    "        mfcc_img_new = mfcc_img_new[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "elif source == 'mic':\n",
    "    duration = 3  # seconds to record\n",
    "    fs = 48000   # Sampling rate\n",
    "    print(f\"Recording audio for {duration} seconds...\")\n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
    "    sd.wait() \n",
    "    y_new = recording.flatten()  \n",
    "    sr_new = fs\n",
    "    print(\"Recording complete.\")\n",
    "\n",
    "    mfcc_record = librosa.feature.mfcc(y=y_new, sr=sr_new, n_mfcc=13)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_record)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc_record, order=2)\n",
    "    mfcc_mean = np.mean(mfcc_record, axis=1)\n",
    "    mfcc_std = np.std(mfcc_record, axis=1)\n",
    "    delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "    delta_std = np.std(mfcc_delta, axis=1)\n",
    "    delta2_mean = np.mean(mfcc_delta2, axis=1)\n",
    "    delta2_std = np.std(mfcc_delta2, axis=1)\n",
    "    \n",
    "    # Concatenate statistical features for Random Forest prediction\n",
    "    features_new = np.concatenate([mfcc_mean, mfcc_std, delta_mean, delta_std, delta2_mean, delta2_std]).reshape(1, -1)\n",
    "\n",
    "    \n",
    "    # For the CNN model, create a fixed-size MFCC image\n",
    "    max_len = 216\n",
    "    mfcc_img_new = librosa.util.fix_length(mfcc_record, size=max_len, axis=1)\n",
    "    mfcc_img_new = mfcc_img_new[np.newaxis, ..., np.newaxis]\n",
    "else:\n",
    "    raise ValueError(\"Invalid source selected. Please set source to 'file' or 'mic'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions using the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [\"Neutral\", \"Calm\", \"Happy\", \"Sad\", \"Angry\", \"Fearful\", \"Disgust\", \"Surprised\"]\n",
    "\n",
    "rf_pred_new = rf_clf.predict(features_new)[0]\n",
    "cnn_pred_probs_new = cnn_model.predict(mfcc_img_new)[0]\n",
    "cnn_pred_new = np.argmax(cnn_pred_probs_new)\n",
    "\n",
    "print(\"Random Forest Prediction:\", emotion_labels[rf_pred_new])\n",
    "print(\"CNN Prediction:\", emotion_labels[cnn_pred_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile a report detailing your challenges faced, and the performance of the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your report Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
